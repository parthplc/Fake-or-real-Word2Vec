{"cells":[{"metadata":{},"cell_type":"markdown","source":"Importing Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"markdown","source":"Importing Dataset"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nsample_submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\ntest = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\ntrain = pd.read_csv(\"../input/nlp-getting-started/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Steps\nThe problems seems to be of three steps:\n* Preprocessing of words\n* Word2Vec training on our dataset\n* Prediction using vector outputted by Word2Vec."},{"metadata":{},"cell_type":"markdown","source":"# Step 1\nText Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nimport re\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'] = train['text'].apply(lambda x: clean_text(x))\ntest['text'] = test['text'].apply(lambda x: clean_text(x))\n\n# Let's take a look at the updated text\ntrain['text'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 2 : Training Word2Vec on our dataset from scratch."},{"metadata":{},"cell_type":"markdown","source":"Well there are two methods of implementation for Word2Vec on a given dataset.\n* Either train from scratch.\n* Or use google pretrained word2vec model.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import Word2Vec, KeyedVectors\nimport nltk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenizing the training and the test set\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntrain['text'] = train['text'].apply(lambda x: tokenizer.tokenize(x))\ntest['text'] = test['text'].apply(lambda x: tokenizer.tokenize(x))\ntrain['text'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets remove the stopwords as it does not seems of any meaning","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords \ndef remove_stopwords(text):\n    \"\"\"\n    Removing stopwords belonging to english language\n    \n    \"\"\"\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'] = train['text'].apply(lambda x : remove_stopwords(x))\ntest['text'] = test['text'].apply(lambda x : remove_stopwords(x))\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let us concatenate both test and train column to get a larger corpus containing larger no of words."},{"metadata":{"trusted":true},"cell_type":"code","source":"test['target'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def combine_text(list_of_text):\n    '''Takes a list of text and combines them into one large chunk of text.'''\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ntrain['text'] = train['text'].apply(lambda x : combine_text(x))\ntest['text'] = test['text'].apply(lambda x : combine_text(x))\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we will make a corpus of words to start word2vec training.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([train,test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = df['text'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Corpus_list = [nltk.word_tokenize(title) for title in corpus]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Corpus_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Word2Vec(Corpus_list,min_count=1,size = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.most_similar('death')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We can also used google pretrained saved model on newspaper as no of trained words much larger corpus and would give more accurate results then this small dataset"},{"metadata":{},"cell_type":"markdown","source":"Importing the pretrained vectors using this link\n\nhttps://www.kaggle.com/umbertogriffo/googles-trained-word2vec-model-in-python"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pwd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"../input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim\nmodel = gensim.models.KeyedVectors.load_word2vec_format(path,binary=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets play around with this 300 sized vector space for all words."},{"metadata":{"trusted":true},"cell_type":"code","source":"w = model[\"hello\"]\nprint(len(w))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(w)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you are very much intrested in this than follow this link\n\nhttps://code.google.com/archive/p/word2vec/"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Now to represent the word we must convert the whole word to something small value of single numerical.So right now "},{"metadata":{},"cell_type":"markdown","source":"So we can see their quite some diffrences between the pretrained vector on our dataset and that on google newspaper dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"class MeanEmbeddingVectorizer(object):\n\n    def __init__(self, word_model):\n        self.word_model = word_model\n        self.vector_size = word_model.wv.vector_size\n\n    def fit(self):  # comply with scikit-learn transformer requirement\n        return self\n\n    def transform(self, docs):  # comply with scikit-learn transformer requirement\n        doc_word_vector = self.word_average_list(docs)\n        return doc_word_vector\n\n    def word_average(self, sent):\n        \"\"\"\n        Compute average word vector for a single doc/sentence.\n\n\n        :param sent: list of sentence tokens\n        :return:\n            mean: float of averaging word vectors\n        \"\"\"\n        mean = []\n        for word in sent:\n            if word in self.word_model.wv.vocab:\n                mean.append(self.word_model.wv.get_vector(word))\n\n        if not mean:  # empty words\n            # If a text is empty, return a vector of zeros.\n            #logging.warning(\"cannot compute average owing to no vector for {}\".format(sent))\n            return np.zeros(self.vector_size)\n        else:\n            mean = np.array(mean).mean(axis=0)\n            return mean\n\n\n    def word_average_list(self, docs):\n        \"\"\"\n        Compute average word vector for multiple docs, where docs had been tokenized.\n\n        :param docs: list of sentence in list of separated tokens\n        :return:\n            array of average word vector in shape (len(docs),)\n        \"\"\"\n        return np.vstack([self.word_average(sent) for sent in docs])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In case you want to use different technique than averaging we can use tfidf using this technique."},{"metadata":{"trusted":true},"cell_type":"code","source":"class TfidfEmbeddingVectorizer(object):\n    def __init__(self, word2vec):\n        self.word2vec = word2vec\n        self.word2weight = None\n        self.dim = len(word2vec.itervalues().next())\n\n    def fit(self, X, y):\n        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n        tfidf.fit(X)\n        # if a word was never seen - it must be at least as infrequent\n        # as any of the known words - so the default idf is the max of \n        # known idf's\n        max_idf = max(tfidf.idf_)\n        self.word2weight = defaultdict(\n            lambda: max_idf,\n            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n\n        return self\n\n    def transform(self, X):\n        return np.array([\n                np.mean([self.word2vec[w] * self.word2weight[w]\n                         for w in words if w in self.word2vec] or\n                        [np.zeros(self.dim)], axis=0)\n                for words in X\n            ])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_vec_tr = MeanEmbeddingVectorizer(model)\ndoc_vec = mean_vec_tr.transform(Corpus_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of word-mean doc2vec...')\ndisplay(doc_vec.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Corpus_train = train['text'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_corpus = [nltk.word_tokenize(title) for title in Corpus_train]\ndoc_vec_1 = mean_vec_tr.transform(train_corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_corpus)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of word-mean doc2vec...')\ndisplay(doc_vec_1.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Corpus_test = test['text'].values\ntest_corpus = [nltk.word_tokenize(title) for title in Corpus_test]\ndoc_vec_2 = mean_vec_tr.transform(test_corpus)\nprint('Shape of word-mean doc2vec...')\ndisplay(doc_vec_2.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = doc_vec_1\ny = train['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\nclf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf,X,y, cv=5, scoring=\"f1\")\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_vec_test = doc_vec_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission_1 = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\nsample_submission_1[\"target\"] = clf.predict(X_vec_test)\nsample_submission_1.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Advance Algorithms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb_model = XGBClassifier()\n\n# #brute force scan for all parameters, here are the tricks\n# #usually max_depth is 6,7,8\n# #learning rate is around 0.05, but small changes may make big diff\n# #tuning min_child_weight subsample colsample_bytree can have \n# #much fun of fighting against overfit \n# #n_estimators is how many round of boosting\n# #finally, ensemble xgboost with multiple seeds may reduce variance\n# parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n#               'objective':['binary:logistic'],\n#               'learning_rate': [0.05,0.01,0.1], #so called `eta` value\n#               'max_depth': [6,7,8,10],\n#               'min_child_weight': [11],\n#               'silent': [1],\n#               'subsample': [0.8],\n#               'colsample_bytree': [0.7,0.6,.5],\n#               'n_estimators': [100,1000], #number of trees, change it to 1000 for better results\n#               'missing':[-999],\n#               'seed': [1337]}\n\n\n# clf = GridSearchCV(xgb_model, parameters, n_jobs=5,  \n#                    scoring='roc_auc',\n#                    verbose=2, refit=True)\n\n# clf.fit(X,y)\n\n# #trust your CV!\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(\"Best parameters set found on development set:\")\n# print()\n# print(clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = XGBClassifier(colsample_bytree=0.7, learning_rate= 0.05, max_depth= 8,\n                    min_child_weight=11, missing= -999, n_estimators= 1000,\n                    nthread= 4, objective='binary:logistic', seed=1337, silent=1, subsample=0.8)\nscores = model_selection.cross_val_score(clf,X,y, cv=5, scoring=\"f1\")\nscores\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission_1 = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\nsample_submission_1[\"target\"] = clf.predict(X_vec_test)\nsample_submission_1.to_csv(\"submission_3.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}